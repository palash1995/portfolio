{
  "main": {
    "name":"Palash Chaudhari",
    "description":"Astronomer’s Apache Airflow Fundamental Certified and Microsoft Certified Azure Data Engineer with 4+ years of experience in Azure, AWS, ETL, Spark and strong expertise in distributed data processing.",
    "image":"profilepic.jpg",
    "bio1": "Palash is a passionate Data Engineer having four year of experience in Spark, Snowflake, Databricks, Airflow, Azure, AWS, SQL, and Python. Over the period, he has worked with different clients in various roles, with the main motivation of building robust and scalable pipelines to serve meaningful data to theend users.",
    "bio2": "During my tenure at both Cognizant and the University of San Francisco, I demonstrated proficiency in transforming data infrastructures and driving impactful insights. I engineered ETL pipelines utilizing Azure Data Factory, PySpark, and SQL to extract data from 118 diverse sources and load it into Synapse. Additionally, I designed key performance indicators for loan lending and repayment, enhancing financial analysis capabilities. At both organizations, I conducted rigorous data quality checks and streamlined workflows, leveraging technologies like Airflow DAGs and Azure DevOps to automate tasks and save significant manual effort. My multifaceted experience in data engineering and analytics has equipped me with a robust skill set to tackle complex challenges and drive meaningful outcomes.",
    "contactmessage":"Please fill the details below and I'll contact you ASAP!",
    "email": "palash.chaudhari95@gmail.com",
    "phone": "(669) 302 5069",
    "github":"https://github.com/palash1995",
    "linkedIn":"https://www.linkedin.com/in/palashchaudhari/",
    "portfolio": "",
    "degree": "Master of Science (M.S.)",
    "address":{
      "city":"San Francisco",
      "state":"CA",
      "country":"USA"
    },
    "social":[
      {
        "name":"linkedin",
        "url":"https://www.linkedin.com/in/palashchaudhari/",
        "icon":"linkedin"
      },
      {
        "name":"github",
        "url":"https://github.com/palash1995",
        "icon":"github"
      },
      {
        "name":"medium",
        "url":"https://medium.com/@palash1995",
        "icon":"medium"
      }
    ],
    "topSkills":[
      {
        "name":"Python",
        "level":"90%"
      },
      {
        "name":"SQL",
        "level":"90%"
      },
      {
        "name":"Spark",
        "level":"85%"
      },
      {
        "name":"ETL",
        "level":"85%"
      },
      {
        "name":"Airflow",
        "level":"80%"
      }
    ]
  },
  "resume":{
    "education":[
      {
        "school":"University of San Francisco, USA",
        "degree":"Master of Science, Information Systems",
        "graduated":"December 2023",
        "gpa": "3.93",
        "description":"Course Work: Data Architecture and Management, Information Architecture, Predictive Analytics, Business Analytics for IS"
      },
      {
        "school":"Savitribai Phule Pune University, India",
        "degree":"Bachelor of Engineering, Electronics and Telecommunications",
        "graduated":"May 2017",
        "gpa": "3.50",
        "description":"Course Work: Data Structures and Algorithms, Computer Organization, System Programming, Object-Oriented Programming."
      }
    ],
    "work":[
      {
        "company":"University of San Francisco",
        "title":"Data Engineer - Internship",
        "years":"March 2023 - November 2023",
        "description":[
          "Transformed data warehouse by implementing dimensional model schema in Synapse, reducing query response time by 50%.",
          "Architected scalable Spark data pipelines using Azure Data Factory, Databricks and Blob storage to extract and process data using REST APIs, resulting standardization of over 400 thousand data monthly.",
          "Designed and implemented a data infrastructure migration strategy, successfully migrating 50TB of data from an on-premise server to the Azure cloud platform, reducing monthly compute and storage cost by 30%.",
          "Improved data availability and refresh rates for essential business dashboards and decision-making processes by implementing automated data ingestion and ETL workflows utilizing SQL Server and SSIS, thereby optimizing operational efficiency.",
          "Integrated Databricks Spark with data lake solutions using ADF, ADLS Gen2, enabling advanced analytics and reducing data processing time for large datasets by 40%.",
          "Leveraged Python and SQL for data quality checks, reducing PII data quality issues from an average of 60 per week to 2-3.",
          "Implemented Medallion Architecture to streamline the transformation process of financial data, subsequently enabling the creation of customized metrics and a Tableau dashboard."
        ]
      },
      {
        "company":"Cognizant",
        "title":"Database Engineer",
        "years":"April 2019 - July 2022",
        "description":[
          "Designed ETL pipelines using Azure Data Factory, PySpark, Azure Blob that extract data from 118 diverse sources across the global region in SQL Server and load into Snowflake.",
          "Implemented Type 1 and Type 2 SCD (Slowly Changing Dimensions) methodologies in dimension tables to maintain historical data integrity and handled the Change Data Capture (CDC) using SSIS.",
          "Implemented data encryption and role-based access control (RBAC) to ensure data governance and compliance, particularly for sensitive financial data, thereby fortifying security measures and mitigating risks.",
          "Optimized SQL queries by implementing clustering keys, efficient joins, and leveraging query profiling tools like Execution Plan to enhance performance and reduce data processing times.",
          "Initiated the use of NoSQL databases with Cosmos DB data consistency models and comprehensive logging capabilities to ensure data integrity and create traceable audit trails, thereby enhancing regulatory compliance and auditing processes within the bank.",
          "Engineered a comprehensive financial strength analysis model by creating cumulative tables and OLAP cubes using SSAS to assess loan lending and repayment performance metrics, driving a 25% improvement in loan portfolio profitability.",
          "Conducted comprehensive data quality checks on over 200 GB of data monthly to enhanced the accuracy of ad-hoc reports.",
          "Orchestrated data processing tasks and streamlined workflows by creating Airflow DAGs to extract regulatory reports, saving over 30 hours of manual effort per week.",
          "Developed and implemented comprehensive data governance policies using Azure Purview, which led to a significant reduction in security incidents by 20% within the first year.",
          "Embedded Machine Learning models seamlessly into fraud detection systems within the production pipelines, yielding a significant 35% improvement in predictive precision for identifying and mitigating fraudulent activities.",
          "Maintained continuous integration and deployment of SQL and Store Procedures scripts by managing CI/CD pipelines using Azure DevOps.",
        ]
      },
      {
        "company":"",
        "title":"Database Engineer",
        "years":"April 2018 - March 2019",
        "description":[
          "Migrated a unified Data Warehouse from Sybase to AWS Redshift, centralizing diverse datasets, reducing yearly cost by 20%.",
          "Authored SSIS packages for processing Fact and Dimension tables with complex transforms, handling Change Data Capture (CDC) and Type-2 SCD, reducing data processing time by 30%.",
          "Deployed SSIS packages to extract data from OLTP to SQL Server Analysis Services (SSAS) and scheduled jobs to call the packages, leading to a 50% reduction in manual workload.",
          "Created SSIS Packages using ETL tools such as SSIS Designer for exporting heterogeneous data from Sybase DB Source, Excel Spreadsheet to SQL Server.",
          "Conducted comprehensive audits of existing BI processes, identifying and rectifying data integrity issues, which resulted in a 20% increase in data accuracy and a significant boost in stakeholder trust.",
          "Optimized SQL performance through partitioning, indexing, clustering, resulting system runtime reduced from 3 to 1.5 hours.",
          "Crafted SSRS reports using SSAS Tabular DAX, Multidimensional cubes (MDX) for streamlining decision-making processes.",
          "Migrated a unified Data house from Sybase IQ to Azure SQL data warehouse, reducing yearly license cost by 20%.",
          "Created SQL scripts to extract and transform complex financial datasets for regulatory reporting, ensuring 100% compliance.",
          "Enforced robust data security measures in compliance with GDPR, leading to 95% audit compliance rate and zero data breaches."
        ]
      }
    ],
    "skillsList": {
      "Programing Languages": "Python, SQL, PySpark, Java",
      "Data Engineering Skills": "Spark, Airflow, Snowflake, Databricks, Data Modeling, ETL, CI/CD, Git",
      "Azure Cloud": "Azure Data Factory, Synapse Analytics, Streaming Analytics, Event Hub, Blob, DevOps pipeline.",
      "AWS Cloud": "S3 Storage, Redshift, DynamoDB, Lambda, Glue, Kinesis, EMR, CloudWatch, Athena, QuickSight.",
      "Databases": "Trino, Presto, MySQL, MS SQL, Oracle, PostgreSQL",
      "File Format": "Parquet, Avro, JSON, ORC, CSV, XML",
      "Project Management" : "Agile, DevOps, JIRA, Scrum, Kanban, ServiceNow, Miro"
    },
    "certifications": [
       {
        "name": "Astronomer Apache Airflow Fundamentals Certification",
        "issuedOn": "March 2024",
        "link": "https://www.credly.com/badges/b4323f1d-5d95-4316-96b0-d36fc04a8420/linked_in_profile"
      },
      {
        "name": "Microsoft Certified Azure Data Engineering Associate (DP-203)",
        "issuedOn": "December 2023",
        "link": "https://learn.microsoft.com/en-gb/users/palashchaudhari-2212/credentials/2ba2eea7a3a2121b?ref=https%3A%2F%2Fwww.linkedin.com%2F"
      },
      {
        "name": "Databricks Lakehouse Fundamentals",
        "issuedOn": "March 2024",
        "link": "https://credentials.databricks.com/f9187413-eb97-4432-9412-1b77abd43f33#gs.6f1rhw"
      },
      {
        "name": "Databricks Generative AI Fundamentals",
        "issuedOn": "March 2024",
        "link": "https://www.credly.com/badges/6a48d1f2-ac6e-4d2a-b965-4c0e7d473420"
      },
      {
        "name": "Business Analytics – Harvard Business School",
        "issuedOn": "June 2023",
        "link": "https://online.hbs.edu/verify-certificate?dvid=W2X7HP8G"
      }
    ],
    "publications": [
      {
        "name": "Redundant Data Normalization using the Novel Data Mining Algorithms",
        "issuedOn": "July - August 2020",
        "link": "https://www.warse.org/IJATCSE/static/pdf/file/ijatcse361942020.pdf"
      }
    ],
    "activities": [
      {
        "name": "Enlighten Rural India - Sampark Village Development and Educational Society",
        "title":"Project Coordinator",
        "years":"August 2017 - February 2018",
        "description":[
          "Facilitated computer and programming education for rural Indian children alongside basic science and mathematics subjects.",
          "Enhanced awareness among them about recent advancements in technology and its implications.",
          "Advocated the significance of education and encouraging them to pursue higher studies and degree programs."
        ]
      }
    ]
  },
  "portfolio":{
    "work": [
      {
        "title":"Data Engineering",
        "description":"Build and maintain robust data pipelines, ensuring the efficient and reliable collection, using ETL tools",
        "image":"data-pipeline.png",
        "url":"https://github.com/palash1995"
      },
      {
        "title":"Data Analysis",
        "description":"Analyze datasets to extract meaningful insights and make data-driven decisions for business",
        "image":"data-science.png",
        "url":"https://github.com/palash1995"
      },
      {
        "title":"Prompt Engineering",
        "description":"Craft precise and context-aware prompts to get optimized responses from AI models.",
        "image":"prompt-engineering.png",
        "url":"https://github.com/palash1995"
      },
      {
        "title":"Data Modeling",
        "description":"Create ML models to uncover patterns and relationships within data, enabling strategic planning for businesses.",
        "image":"software-development.png",
        "url":"https://github.com/palash1995"
      }
    ],
    "projects": [
      {
        "title":"Real-time Wrist Watch Data Processing with Kafka and Spark",
        "description":"Developed real-time health data monitoring and analysis systems utilizing Kafka and Spark within Azure Databricks. These systems receive continuous data streams from wristwatches and leverage Medallion data Lakehouse architecture for efficient organization and processing of the data.",
        "image":"video-interview.png",
        "url":"https://github.com/palash1995/Real-time-Wrist-Watch-Data-Processing-with-Kafka-and-Spark",
        "techStack":[]
      },
      {
        "title":"Advertisement Impact Analysis using AWS Cloud and Snowflake",
        "description":"Deployed an AWS-driven streaming pipeline to analyze the real-time effects of marketing campaigns. Data generated in real-time was stored in Snowflake for analysis of campaign performance. Python was employed for analytics tasks, while AWS tools like Lambda, Kinesis, S3, and SQS facilitated seamless data processing.",
        "image":"stock-close.png",
        "url":"https://github.com/palash1995/Advertisement-Impact-Analysis-using-AWS-Cloud-and-Snowflake",
        "techStack":[]
      },
      {
        "title":"Predictive Analytics and Reporting for COVID-19 using Azure Data Factory",
        "description":"Engineered a end-to-end pipeline on Azure Data Factory for predictive analytics and reporting concerning COVID-19. This pipeline facilitated the gathering, processing, and analysis of COVID-19 data. I created insightful visualizations using Power BI to present the findings and insights derived from the data.",
        "image":"flight-booking.png",
        "url":"https://github.com/palash1995/Predictive-Analytics-and-Reporting-for-COVID-19-using-Azure-Data-Factory",
        "techStack":[]
      },
      {
        "title":"AWS based ETL pipeline for Music platforms",
        "description":"Designed ETL pipeline to extracted the top 100 songs from Spotify, Amazon and Apple Music on a weekly basis utilizing using APIs. Leveraging Glue, I automated the cataloging process and inferred schemas, while Athena was employed for in-depth data analysis. QuickSight was utilized for visually presenting the music trends.",
        "image":"cms.png",
        "url":"https://github.com/palash1995/AWS-based-ETL-pipeline-for-Music-platforms",
        "techStack":[]
      },
      {
        "title":"FOREX Data Pipeline using Airflow",
        "description":"Built the Forex Data Pipeline utilized Airflow to ingesting, processing, and analysis of foreign exchange data. It streamlined the collection of real-time market data, enabling traders to make informed decisions based on up-to-date insights, enhancing their ability to navigate the dynamic currency trading landscape effectively.",
        "image":"malware-detection.png",
        "url":"https://github.com/palash1995/FOREX-Data-Pipeline-using-Airflow",
        "techStack":[]
      },
      {
        "title":"Customer Review Analysis",
        "description":"Analyzed customer feedback for British Airways entails gathering data, cleaning it, visualizing trends, and using advanced transformer models to classify sentiment. This process provides valuable insights into customer opinions, enhancing understanding of their experiences. Used Python and Jupyter notebook.",
        "image":"ba-review.png",
        "url":"https://github.com/palash1995/Customer-Review-Analysis",
        "techStack":[]
      }
    ]
  }
}
